---
title: "Relationship between dictionary count of emotion words and mood disorder incidence"
author: "Insight Out"
date: "06/10/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```


```{r libraries}
library(dplyr)
library(tidyverse)
library(here)
library(DataCombine)
library(lme4)
```


Specify paths for each of the files and load them
```{r loaddata, include=TRUE}
dict_path <- here("data", "capstone20_dictionaries.csv")  
adjcount_path <- here("data", "dictionary_word_matrix_adj.csv")  
countries_path <- here("data", "countries-languages.csv")
disease_path <- here("data", "disease-incidences.csv")
cluster_path <- here("data", "cultural-clusters.csv")
population_path <- here("data", "world-population.csv")

# List of dictionaries in dataset
dict <- read_csv(dict_path)  

# List of countries and the languages they speak
lang_countries <- read_csv(countries_path) 

# Mental health disorder prevalance by country
mh <- read_csv(disease_path) 

# World population by country
population <- read.csv(population_path)
```

Positive and negative affect words from PANAS X
```{r loadwords, include=TRUE}
positive_words <- c("active","alert","attentive","enthusiastic","excited","interested","proud","happy", "joyful","delighted","cheerful","lively","energetic","strong","confident","bold","daring","fearless")
negative_words <- c("afraid","scared","nervous","jittery","irritable","hostile","guilty","ashamed","upset", "distressed","frightened","shaky","angry","disgusted","sad","lonely")

```


Calculate mean frequencies for positive and negative affect
```{r calculatefrequencies, include=TRUE}
# load adj matrix and sum across columns to compute the total size of each dictionary
adjcount <- read_csv(adjcount_path) %>%  
  mutate(dictsize = select(., abbreviated:zealous) %>% apply(1, sum, na.rm=TRUE))

word_count <- adjcount %>% 
  select(id, all_of(positive_words), all_of(negative_words), dictsize) %>% mutate_all(~replace_na(.x, 0))

# we'll express frequencies in terms of counts per 10000 tokens
unit <- 10000

# 
word_frequencies <- dict %>% 
  inner_join(word_count, by="id") %>%
  filter(dictsize >= 5000)  # only include dictionaries with more than 5000 tokens

# generate frequencies
for (word in c(positive_words, negative_words)){
  word_frequencies <- word_frequencies %>% mutate(!!sym(word) :=  unit * !!sym(word)/dictsize)
}

# calculate mean frequencies
word_frequencies$mean_positive<-rowSums(word_frequencies[,positive_words])
word_frequencies$mean_negative<-rowSums(word_frequencies[,negative_words])

# take the mean of the frequencies for each dictionary in order to merge into a single language
panas_frequencies <- aggregate(word_frequencies, by = list(language = word_frequencies$langname), FUN = mean)

#select relevant columns
panas_frequencies <- panas_frequencies %>% select(language, mean_positive, mean_negative)
```


Replace Language Names to match CIA dataset to Dictionary Dataset
```{r replacelanguagenames, include=TRUE}
replaces <- data.frame(from = c("Albanian", "Bosnian", "Khmer", "Croatian", "Serbian", "Arabic", "Mandarin", "Persian", "Mongolian", "Greek", "Hebrew", "Armenian", "Malay", "Kyrgyz"), to = c("Northern Tosk Albanian", "Serbian-Croatian-Bosnian", "Central Khmer", "Serbian-Croatian-Bosnian", "Serbian-Croatian-Bosnian", "Standard Arabic", "Mandarin Chinese", "Western Farsi", "Halh Mongolian", "Modern Greek", "Modern Hebrew", "Eastern Armenian", "Standard Malay", "Kirghiz"))
lang_countries <- FindReplace(data = as.data.frame(lang_countries), Var = "Language", replaceData = replaces,
                     from = "from", to = "to", exact = TRUE)

```

Merge CIA data with Cultural Cluster data to include cluster
```{r addsubregions, include=TRUE}
cultural_cluster <- read_csv(cluster_path)
cul_lang <- merge(x= cultural_cluster, y = lang_countries, by.x = "country", by.y = "Country")

```


Merge IMHE data with CIA data to obtain disorder prevelance by countries and languages spoken
```{r mergedata, include=TRUE}
# Obtain Depressive Disorder Prevalence by Country
replaces2 <-data.frame(from = c("Myanmar", "Congo", "Czech Republic", "Russian Federation", "Taiwan, Province of China", "Virgin Islands, U.S."), to = c("Burma", "Republic of the Congo", "Czechia", "Russia", "Taiwan", "Virgin Islands"))

mh <- FindReplace(data = as.data.frame(mh), Var = "location", replaceData = replaces2,
                     from = "from", to = "to", exact = TRUE)

population <- FindReplace(data = as.data.frame(population), Var = "location_name", replaceData = replaces2,
                     from = "from", to = "to", exact = TRUE)

incidence <- mh %>%   select(location, val, cause) %>% 
  filter(!cause == "Dysthymia")%>%
  rename(incidence_count = val)

world_pop <- population %>% 
  filter(age_group_id == "22" & year_id == "2017" & sex_id =="3") %>%
  rename(total_pop = val)%>%
  filter(total_pop != 10502507) %>%    #remove georgia, US
  select(location_name, total_pop)

disease_pop <- merge( x= incidence, y=world_pop, 
                      by.x = "location", 
                      by.y = "location_name")

cul_lang_pop <- merge(x= cul_lang, y = disease_pop, 
                      by.x = "country", 
                      by.y = "location")

```


Merge temporary dataset with panas frequencies
```{r createdataframe, include=TRUE}
d <- merge(x= cul_lang_pop, y = panas_frequencies, by.x = "Language", by.y = "language")
```

```{r test2, include=TRUE}
# weigh each country by the percentage of people speaking the particular language
d <- d %>% group_by(country) %>% 
  mutate(mean_positive = weighted.mean(mean_positive, Percentage/100)) %>% 
  mutate(mean_negative = weighted.mean(mean_negative, Percentage/100)) %>% 
  mutate(Percentage = sum(Percentage)/2) %>% 
  subset(select = -c(Language)) %>% 
  distinct()

# remove countries with percentage < 80
d <- d %>% subset(Percentage >= 80) %>% subset(select = -c(Percentage))

#pivot longer
d <- d %>% pivot_wider(names_from = "cause", values_from="incidence_count")

# rename columns
d <- rename(d, c("major_depressive_disorder"="Major depressive disorder",  "anxiety_disorders" = "Anxiety disorders"))

# Remove GEORGIA
d <- d %>% filter(country != "Georgia" | total_pop < 5000000)

# Create Total
d <- d %>% mutate(total = (mean_positive + mean_negative))

#Round population and disorder incidence 
d <- d %>% mutate(total_pop = round(total_pop), anxiety_disorders = round(anxiety_disorders), major_depressive_disorder = round(major_depressive_disorder)) 

```


## Exploratory Plot
Draw graph for depressive disorders (all three disorders combined)
```{r plotgraphs, include=TRUE}

plotd <- d %>% 
  pivot_longer(cols=c(mean_positive, mean_negative), names_to="affect_words", values_to="propn") %>% 
  mutate(percentage_mdd= 100 * major_depressive_disorder/total_pop) %>%
  mutate(percentage_anxiety= 100* anxiety_disorders/total_pop) 
  
ggplot(plotd, aes(y=percentage_mdd, x=total, color=cluster)) +
  geom_point() +
  theme_bw() +
  geom_smooth(color="black", method=lm) +
  labs(y="Major Depressive Disorder prevalence/%", x="Emotion words per 10000 tokens", title="Scatterplot of emotion words and Major Depressive Disorder prevalence")

ggplot(plotd, aes(y=percentage_anxiety, x=total, color=cluster)) +
  geom_point() +
  theme_bw() +
  geom_smooth(color="black", method=lm) +
  labs(y="Anxiety disorders prevalence/%", x="Emotion words per 10000 tokens", title="Scatterplot of emotion words and Anxiety disorders prevalence")
```



## Correlations
```{r correlation, include=TRUE}
da <- d %>%  mutate(percentage_mdd=100 * major_depressive_disorder/total_pop) %>%
  mutate(percentage_anxiety= 100 * anxiety_disorders/total_pop) 

cor.test(da$total, da$percentage_mdd)
cor.test(da$total, da$percentage_anxiety)



```

##Statistical Analysis - Mixed Model Regression
```{r MD regression, include=TRUE}
# We will not used linear regression in our lab report because one of the assumption in linear regression is independence of data, and our country data violate this assumption. Instead, we will use mixed model regression. We found a study that group countries based on cultural clusters (Mensah, Y. M., & Chen, H. Y. (2013). Global clustering of countries by cultureâ€“an extension of the GLOBE study.) and we will use this cultural cluster as our random effect for the analysis. 

FullDepressionModel <- glmer(cbind(major_depressive_disorder, total_pop - major_depressive_disorder) ~ total + (1|cluster), data = d, family = binomial, control=glmerControl(optimizer="bobyqa"), nAGQ=0)
beta_temp <- round(coef(summary(FullDepressionModel))[2,"Estimate"], 2)
show(FullDepressionModel)

ReducedDepressionModel <- glmer(cbind(major_depressive_disorder, total_pop - major_depressive_disorder) ~ (1|cluster), data = d, family = binomial, control=glmerControl(optimizer="bobyqa"))

a1<-anova(FullDepressionModel,ReducedDepressionModel)
p_value <-  signif(a1$"Pr(>Chisq)"[2] , digits=3)
chi_sq <- signif(a1$Chisq[2], digits=3)
df <- a1$Df[2]
show(a1)

```

```{r AD regression}
FullADModel <- glmer(cbind(anxiety_disorders, total_pop - anxiety_disorders) ~ total + (1|cluster), data = d, family = binomial, control=glmerControl(optimizer="bobyqa", nAGQ=0))
beta_temp <- round(coef(summary(FullADModel))[2,"Estimate"], 2)
show(FullADModel)

ReducedADModel <- glmer(cbind(anxiety_disorders, total_pop - anxiety_disorders) ~ (1|cluster), data = d, family = binomial, control=glmerControl(optimizer="bobyqa"))

a1<-anova(FullADModel,ReducedADModel)
p_value <-  signif(a1$"Pr(>Chisq)"[2] , digits=3)
chi_sq <- signif(a1$Chisq[2], digits=3)
df <- a1$Df[2]
show(a1)
```


## Control Independant Variable
Select 36 random words and perform same method to compare with PANAS words
```{r calculatecontrol}

#calculate_significance <- function(){
  random_words <- colnames(sample(select(adjcount, abbreviated:zealous), size=36))
  
  control_word_count <- adjcount %>% 
    select(id, random_words, dictsize) %>% mutate_all(~replace_na(.x, 0))
  
  # we'll express frequencies in terms of counts per 10000 tokens
  unit <- 10000
  
  #
  control_word_frequencies <- dict %>% 
    inner_join(control_word_count, by="id") %>%
    filter(dictsize >= 5000)  # only include dictionaries with more than 5000 tokens
  
  # generate frequencies
  for (word in random_words){
    control_word_frequencies <- control_word_frequencies %>% mutate(!!sym(word) :=  unit * !!sym(word)/dictsize)
  }
  
  # calculate mean frequencies
  control_word_frequencies$mean_words<-rowSums(control_word_frequencies[,random_words])
  
  # take the mean of the frequencies for each dictionary in order to merge into a single language
  control_frequencies <- aggregate(control_word_frequencies, by = list(language = control_word_frequencies$langname), FUN = mean)
  
  #select relevant columns
  control_frequencies <- control_frequencies %>% select(language, mean_words)
  
  d_control <- merge(x= cul_lang_pop, y = control_frequencies, by.x = "Language", by.y = "language")

  # weigh each country by the percentage of people speaking the particular language
  d_control <- d_control %>% group_by(country) %>% 
    mutate(mean_words = weighted.mean(mean_words, Percentage/100)) %>% 
    mutate(Percentage = sum(Percentage)) %>% 
    subset(select = -c(Language)) %>% 
    distinct()
  
  # remove countries with percentage < 80
  d_control <- d_control %>% subset(Percentage >= 80) %>% subset(select = -c(Percentage))
  
  #pivot longer
  d_control <- d_control %>% pivot_wider(names_from = "cause", values_from="incidence_count")
  
  
  # rename columns
  d_control <- rename(d_control, c("major_depressive_disorder"="Major depressive disorder",  "anxiety_disorders" = "Anxiety disorders"))
  
  # Remove GEORGIA
  d_control <- d_control %>% filter(country != "Georgia" | total_pop < 5000000)
  
  #Round population and disorder incidence 
  d_control <- d_control %>% mutate(total_pop = round(total_pop), anxiety_disorders = round(anxiety_disorders), major_depressive_disorder = round(major_depressive_disorder)) 
  
  
  d_control <- d_control %>%  mutate(percentage_mdd=100 * major_depressive_disorder/total_pop) %>%
  mutate(percentage_anxiety= 100 * anxiety_disorders/total_pop) 

ggplot(d_control, aes(x=percentage_mdd, y=mean_words)) +
  geom_point() +
  geom_smooth(method=lm) +
  labs(x="major depressive disorder prevalence/%", y="mean positive")
  
    FullDepressionModel <- glmer(cbind(major_depressive_disorder, total_pop - major_depressive_disorder) ~ mean_words + (1|cluster), data = d_control, family = binomial, control=glmerControl(optimizer="bobyqa"))

  ReducedDepressionModel <- glmer(cbind(major_depressive_disorder, total_pop - major_depressive_disorder) ~ (1|cluster), data = d_control, family = binomial, control=glmerControl(optimizer="bobyqa"))
  
  a1<-anova(FullDepressionModel,ReducedDepressionModel)
  p_value_depression <-  signif(a1$"Pr(>Chisq)"[2] )
  
  
    FullADModel <- glmer(cbind(anxiety_disorders, total_pop - anxiety_disorders) ~ mean_words + (1|cluster), data = d_control, family = binomial, control=glmerControl(optimizer="bobyqa"))

  ReducedADModel <- glmer(cbind(anxiety_disorders, total_pop - anxiety_disorders) ~ (1|cluster), data = d_control, family = binomial, control=glmerControl(optimizer="bobyqa"))
  
  a1<-anova(FullADModel,ReducedADModel)
  p_value_anxiety <-  signif(a1$"Pr(>Chisq)"[2])

  print(p_value_depression)
  print(p_value_anxiety)

```
