---
title: "PANAS by country preliminary analysis"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```


```{r libraries}
library(dplyr)
library(tidyverse)
library(here)
library(DataCombine)
```

Specify paths for each of the files and load them
```{r loaddata, include=TRUE}
dict_path <- here("data", "capstone20_dictionaries.csv")  
adjcount_path <- here("data", "dictionary_word_matrix_adj.csv")  
countries_path <- here("data", "countries-languages.csv")
mh_path <- here("data", "country-mentalhealth.csv")

# List of dictionaries in dataset
dict <- read_csv(dict_path)  

# List of countries and the languages they speak
lang_countries <- read_csv(countries_path) 

# Mental health disorder prevalance by country
mh <- read_csv(mh_path) 
```

Positive and negative affect words from PANAS X
```{r loadwords, include=TRUE}
positive_words = c("active","alert","attentive","enthusiastic","excited","interested","proud","happy", "joyful","delighted","cheerful","lively","energetic","strong","confident","bold","daring","fearless")
negative_words <- c("afraid","scared","nervous","jittery","irritable","hostile","guilty","ashamed","upset", "distressed","frightened","shaky","angry","disgusted","sad","lonely")
```



Calculate mean frequencies for positive and negative affect
```{r calculatefrequencies, include=TRUE}
# load adj matrix and sum across columns to compute the total size of each dictionary
adjcount <- read_csv(adjcount_path) %>%  
  mutate(dictsize = select(., abbreviated:zealous) %>% apply(1, sum, na.rm=TRUE))

word_count <- adjcount %>% 
  select(id, positive_words, negative_words, dictsize) %>% mutate_all(~replace_na(.x, 0))

# we'll express frequencies in terms of counts per 10000 tokens
unit <- 10000

# 
word_frequencies <- dict %>% 
  inner_join(word_count, by="id") %>%
  filter(dictsize >= 5000)  # only include dictionaries with more than 5000 tokens

# generate frequencies
for (word in c(positive_words, negative_words)){
  word_frequencies <- word_frequencies %>% mutate(!!sym(word) :=  unit * !!sym(word)/dictsize)
}

# calculate mean frequencies
word_frequencies$mean_positive<-rowSums(word_frequencies[,positive_words])
word_frequencies$mean_negative<-rowSums(word_frequencies[,negative_words])

# take the mean of the frequencies for each dictionary in order to merge into a single language
panas_frequencies <- aggregate(word_frequencies, by = list(language = word_frequencies$langname), FUN = mean)

#select relevant columns
panas_frequencies <- panas_frequencies %>% select(language, mean_positive, mean_negative)
```


Replace Language Names to match CIA dataset to Dictionary Dataset
```{r replacelanguagenames, include=TRUE}
replaces <- data.frame(from = c("Albanian", "Bosnian", "Khmer", "Croatian", "Serbian", "Arabic", "Mandarin", "Persian", "Mongolian", "Greek", "Hebrew", "Armenian", "Malay", "Kyrgyz"), to = c("Northern Tosk Albanian", "Serbian-Croatian-Bosnian", "Central Khmer", "Serbian-Croatian-Bosnian", "Serbian-Croatian-Bosnian", "Standard Arabic", "Mandarin Chinese", "Western Farsi", "Halh Mongolian", "Modern Greek", "Modern Hebrew", "Eastern Armenian", "Standard Malay", "Kirghiz"))
lang_countries <- FindReplace(data = as.data.frame(lang_countries), Var = "Language", replaceData = replaces,
                     from = "from", to = "to", exact = TRUE)

```


Merge IMHE data with CIA data to obtain disorder prevelance by countries and languages spoken
```{r createtemp, include=TRUE}
# Obtain Depressive Disorder Prevalence by Country
depression_prevalence <- mh %>%   select(location, val, cause)

temp1 <- merge(x= depression_prevalence, y = lang_countries, by.x = "location", by.y = "Country")

```


Merge temporary dataset with panas frequencies
```{r createdataframe, include=TRUE}
d <- merge(x= temp1, y = panas_frequencies, by.x = "Language", by.y = "language")

# weigh each country by the percentage of people speaking the particular language
d <- d %>% group_by(location) %>% mutate(mean_positive = weighted.mean(mean_positive, Percentage/100)) %>% mutate(mean_negative = weighted.mean(mean_negative, Percentage/100)) %>% mutate(Percentage = sum(Percentage)/4) %>% subset(select = -c(Language)) %>% distinct()

# remove countries with percentage < 80
d <- d %>% subset(Percentage >= 80) %>% subset(select = -c(Percentage))

# convert val to percentage
d <- d %>% mutate(val = val*100)


# add ratio column and total column
d <- d %>% mutate(ratio = mean_negative / mean_positive)
d <- d %>% mutate(total = mean_negative + mean_positive)

#pivot longer
d <- d %>% pivot_wider(names_from = "cause", values_from="val")

# rename columns
d <- rename(d, c("depressive_disorders"="Depressive disorders", "dysthymia"="Dysthymia",  "major_depressive_disorder"="Major depressive disorder",  "anxiety_disorders" = "Anxiety disorders"))
```

## Exploratory Plot
Draw graph for depressive disorders (all three disorders combined)
```{r plotgraphs, include=TRUE}

plotd <- d %>% 
  pivot_longer(cols=c(mean_positive, mean_negative, ratio, total), names_to="affect_words", values_to="propn") %>% 
  mutate(affect_words = recode(affect_words, mean_positive="positive words", mean_negative="negative words", ratio="ratio of negative to positive", total="total of negative and positive")) 


ggplot(plotd, aes(x=depressive_disorders, y=propn)) +
  geom_point() +
  facet_wrap(~affect_words, scales = "free") +
  geom_smooth(method=lm) +
  labs(x="depressive disorder prevalence/%", y="counts per 1000, ratio")

```



## Further Plots
Draw graphs for other depressive disorders
```{r plotgraphs2, include=TRUE}
ggplot(plotd, aes(x=dysthymia, y=propn)) +
  geom_point() +
  facet_wrap(~affect_words, scales = "free") +
  geom_smooth(method=lm) +
  labs(x="dysthymia prevalence/%", y="counts per 1000, ratio")


ggplot(plotd, aes(x=major_depressive_disorder, y=propn)) +
  geom_point() +
  facet_wrap(~affect_words, scales = "free") +
  geom_smooth(method=lm) +
  labs(x="major depressive disorder prevalence/%", y="counts per 1000, ratio")


ggplot(plotd, aes(x=anxiety_disorders, y=propn)) +
  geom_point() +
  facet_wrap(~affect_words, scales = "free") +
  geom_smooth(method=lm) +
  labs(x="anxiety disorder prevalence/%", y="counts per 1000, ratio")
```


## Control Independant Variable
Select 36 random words and perform same method to compare with PANAS words
```{r calculatecontrol, include=TRUE}

calculate_significance <- function(){
  random_words <- colnames(sample(select(adjcount, abbreviated:zealous), size=36))
  
  control_word_count <- adjcount %>% 
    select(id, random_words, dictsize) %>% mutate_all(~replace_na(.x, 0))
  
  # we'll express frequencies in terms of counts per 10000 tokens
  unit <- 10000
  
  #
  control_word_frequencies <- dict %>% 
    inner_join(control_word_count, by="id") %>%
    filter(dictsize >= 5000)  # only include dictionaries with more than 5000 tokens
  
  # generate frequencies
  for (word in random_words){
    control_word_frequencies <- control_word_frequencies %>% mutate(!!sym(word) :=  unit * !!sym(word)/dictsize)
  }
  
  # calculate mean frequencies
  control_word_frequencies$mean_words<-rowSums(control_word_frequencies[,random_words])
  
  # take the mean of the frequencies for each dictionary in order to merge into a single language
  control_frequencies <- aggregate(control_word_frequencies, by = list(language = control_word_frequencies$langname), FUN = mean)
  
  #select relevant columns
  control_frequencies <- control_frequencies %>% select(language, mean_words)
  
  d_control <- merge(x= temp1, y = control_frequencies, by.x = "Language", by.y = "language")
  
  # weigh each country by the percentage of people speaking the particular language
  d_control <- d_control %>% group_by(location) %>% mutate(mean_words = weighted.mean(mean_words, Percentage/100)) %>% mutate(Percentage = sum(Percentage)/4) %>% subset(select = -c(Language)) %>% distinct()
  
  # remove countries with percentage < 80
  d_control <- d_control %>% subset(Percentage >= 80) %>% subset(select = -c(Percentage))
  
  # convert val to percentage
  d_control <- d_control %>% mutate(val = val*100)
  
  #pivot longer
  d_control <- d_control %>% pivot_wider(names_from = "cause", values_from="val")

  # rename columns
  d_control <- rename(d_control, c("depressive_disorders"="Depressive disorders", "dysthymia"="Dysthymia", "major_depressive_disorder"="Major depressive disorder",  "anxiety_disorders" = "Anxiety disorders"))
  
  mdd_result <- cor.test(d_control$mean_words, d_control$major_depressive_disorder)
  anxiety_result <- cor.test(d_control$mean_words, d_control$anxiety_disorders)
  return (c(mdd_result$p.value<0.05, anxiety_result$p.value<0.05))
}

mdd_sig <- 0
mdd_not_sig <- 0
anxiety_sig <- 0
anxiety_not_sig <- 0
for(i in 0:9999){
  result <- calculate_significance()
  if (result[1] == TRUE){
    mdd_sig <- mdd_sig + 1
  }
  else{
    mdd_not_sig <- mdd_not_sig +1
  }
  if (result[2] == TRUE){
    anxiety_sig <- anxiety_sig +1
  }
  else{
    anxiety_not_sig <- anxiety_not_sig + 1
  }
}

 print(mdd_sig/mdd_not_sig)
 print(anxiety_sig/anxiety_not_sig)
  

```


Further tests can be done by calculating the chance of a significant result using 36 random words from the dictionary dataset over thousands of sets of random words